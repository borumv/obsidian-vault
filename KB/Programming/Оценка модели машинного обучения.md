---
created: Saturday 8th April 2023 10:26
Last modified: Saturday 8th April 2023 10:26
Aliases: Evaluating a model
Tags: programming
---

Это процесс оценки качества работы модели на новых данных, которые не использовались при обучении модели. Этот этап необходим, чтобы понимать, насколько хорошо модель справляется с задачей и чтобы сравнить результаты разных моделей, чтобы выбрать лучшую из них.

Для оценки качества прогноза модели, есть 3 API:
- **Estimator score method** - Метод `score()` оценщика: у оценщиков есть метод оценки, предоставляющий критерий оценки по умолчанию для проблемы, которую они призваны решить.метод score возвращает одну метрику, которая зависит от типа задачи машинного обучения. Например, для задачи *[[Модель классификации|классификатор]]* метод score возвращает точность ([[метрика точности (accuracy)|accuracy]]) модели на тестовых данных, а для задачи *[[Модель регрессии|регрессии]]* - *[[Коэффициент детерминации (R-squared)|коэффициент детерминации]]* (R-squared).
Пример использования метода *score()* для оценки качества модели *классификации* на тестовых данных:
```python
from sklearn.linear_model import LogisticRegression
# Создание модели логистической регрессии
model = LogisticRegression()
# Обучение модели на обучающих данных
model.fit(X_train, y_train)
# Оценка качества модели на тестовых данных
accuracy = model.score(X_test, y_test)
print("Accuracy:", accuracy)
```
	
- **Using `scoring` parameter** - Оценка модели с помощью scoring параметра - это процесс оценки производительности модели на заданном наборе данных, используя метрику качества, заданную в scoring параметре. Для этого можно использовать функции [[Cross validation|кросс-валидации]]
- **Using functions** - Можно импортировать каждую из метрик отдельно как функцию. 